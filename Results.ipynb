{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "# Executive Summary\n",
    "How many times have you opened up a browser for a random subreddit only to find that it wasn't the random subreddit you were looking for?  We've all been there.  Furthermore, what about when you wonder \"golly, just how similar are different subreddits that are focused one concept but from entirely different points of view?\"  Well, we hear you.  We've scrapped data from two active subreddits which focus around sexuality and using them build a model that's able to detect if it's one subreddit or the other with over an 80% certainty.  Furthermore, if future exploritory data analysis, we hope to one day be able to talk about the defining features of each subculter that's being represented by these subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dsi/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that scrapes a subreddit and turns it into a pandas dataframe.\n",
    "Followed by it being used for the actuallesbians and Braincels subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit(the_subreddit, pages = 40):\n",
    "    all_posts = []\n",
    "    first_url = 'http://www.reddit.com/r/' + the_subreddit + '.json'\n",
    "    url = first_url\n",
    "    list_of_df = []\n",
    "    \n",
    "    # Get Sanity Check:\n",
    "    quick_check = requests.get(first_url, headers = {'User-agent':'Electronic Goddess'})\n",
    "    if int(str(quick_check)[11:14]) == 200:\n",
    "        print(\"Get request successful.\")\n",
    "        time.sleep(3)\n",
    "        print(\"Initiating Scrape...\")\n",
    "    else:\n",
    "        print(\"Get request not 200, instead recieved:\" + str(quick_check))\n",
    "        return\n",
    "    \n",
    "    # Scraping:\n",
    "    for round in range(pages):\n",
    "        try:\n",
    "            res = requests.get(url, headers = {'User-agent':'Electronic Goddess'})\n",
    "            data = res.json()\n",
    "            list_of_posts = data['data']['children']\n",
    "            all_posts = all_posts + list_of_posts\n",
    "            after = data['data']['after']\n",
    "            url = first_url +'?after=' + after\n",
    "            print('Round: '+ str(round + 1))\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print('Limit likely hit.  Returning available posts.')\n",
    "            break\n",
    "#        return all_posts # This can be un-commented out for a straight forward raw scrape\n",
    "\n",
    "    # Formats the parts we care about into a list of dictionaries that'll become the dataframe\n",
    "    for i in range(len(all_posts)):\n",
    "        index_dictionary = {\n",
    "                'title' : all_posts[i]['data']['title'],\n",
    "                'selftext': all_posts[i]['data']['selftext'],\n",
    "                'subreddit' : all_posts[i]['data']['subreddit']\n",
    "            }\n",
    "        list_of_df.append(index_dictionary)\n",
    "    return pd.DataFrame(list_of_df, columns = ['title','selftext','subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Scraped, saved and available to be loaded from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_lesbians = scrape_reddit('actuallesbians')\n",
    "# df_incels = scrape_reddit('braincels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [],
   "source": [
    "# Export to csv (Commented out to avoid re-saving errors)\n",
    "#df_lesbians.to_csv('actuallesbians_9_9_400', index=False)\n",
    "#df_incels.to_csv('braincels_9_9_400', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from CSV\n",
    "df_lesbians = pd.read_csv('./Laboritory/Data/actuallesbians_9_9_400')\n",
    "df_incels = pd.read_csv('./Laboritory/Data/braincels_9_9_400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "# Natural Language Processing\n",
    "Using CountVectorizer to generate features from the post text and title of posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations of the tokenizer, lemmatizer and Count Vectorizer (with preprocessor)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if len(word) > 1 and not word in stop_words])\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       tokenizer = tokenizer.tokenize,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english',\n",
    "                       min_df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining and altering the dataframes to be modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the y Values\n",
    "df_lesbians['is_lesbians'] = 1\n",
    "df_incels['is_lesbians'] = 0\n",
    "\n",
    "# Concatination of the two subreddits\n",
    "les_or_inc = pd.concat([df_lesbians.drop('subreddit', axis=1),\n",
    "                        df_incels.drop('subreddit', axis=1)])\n",
    "\n",
    "# Filling Nulls\n",
    "les_or_inc.fillna('', inplace=True)\n",
    "\n",
    "# Combining the title and selftext columns\n",
    "les_or_inc['all_text'] = les_or_inc['title'] + ' ' + les_or_inc['selftext']\n",
    "\n",
    "# Resetting the Index\n",
    "les_or_inc.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploritory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Cvec DataFrame of both forums\n",
    "df_words = pd.DataFrame(cvec.fit_transform(les_or_inc['all_text']).todense(), \n",
    "                        columns=cvec.get_feature_names())\n",
    "\n",
    "# Inserting the target column\n",
    "df_words['is_lesbians'] = les_or_inc['is_lesbians']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated to Lesbians subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lesbian       0.263913\n",
       "really        0.201454\n",
       "gay           0.200222\n",
       "know          0.183887\n",
       "like          0.172065\n",
       "girlfriend    0.166913\n",
       "feel          0.157275\n",
       "straight      0.156200\n",
       "time          0.155890\n",
       "long          0.147156\n",
       "ago           0.144012\n",
       "week          0.142556\n",
       "thing         0.141777\n",
       "friend        0.138071\n",
       "tell          0.136536\n",
       "little        0.136055\n",
       "cute          0.135052\n",
       "pretty        0.134702\n",
       "month         0.132970\n",
       "Name: is_lesbians, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the correlations to the two data frames.\n",
    "# 1 = represents coming from lesbians subreddit.\n",
    "# 0 = represents coming from incels subreddit.\n",
    "df_corrs = df_words.corr().sort_values(['is_lesbians'])['is_lesbians']\n",
    "print(\"Most correlated to Lesbians subreddit\")\n",
    "df_corrs.tail(20)[18::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most correlated to Incels subreddit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chad               -0.221560\n",
       "incel              -0.185280\n",
       "incels             -0.154910\n",
       "oneitis            -0.143180\n",
       "ugly               -0.142970\n",
       "cope               -0.124894\n",
       "blackpill          -0.118361\n",
       "bro                -0.105852\n",
       "jfl                -0.105735\n",
       "ascend             -0.105704\n",
       "braincels          -0.100735\n",
       "fuck               -0.100300\n",
       "black              -0.097455\n",
       "blackpillcentral   -0.097165\n",
       "normies            -0.096021\n",
       "pill               -0.090948\n",
       "subhuman           -0.090072\n",
       "reminder           -0.088896\n",
       "normie             -0.088658\n",
       "beta               -0.088407\n",
       "Name: is_lesbians, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Most correlated to Incels subreddit\")\n",
    "df_corrs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at words most correlated to one subreddit or the other we can infer what these forums have most different from eachother. Some of which were obvious, such as words being associated with sexuality having a high correlation to the lesbian forum, but others are more odd, like how the use of words like \"month\", \"time\", \"week\" and \"ago\" seeming to point to a higher mention of recent or future timeframes when compaired to the incels. For the incels this seems to have sifted those words around thier specific ingroup terminology such as \"chad\", \"oneitis\" and \"blackpill\". \n",
    " - Planned: Topic Modeling and Word2Vec/Doc2Vec functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the X, y, tests and trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "X = les_or_inc['all_text']\n",
    "y = les_or_inc['is_lesbians']\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=76)\n",
    "\n",
    "# Count Vectorizing the train and test X's while fitting the Training X\n",
    "X_train = pd.DataFrame(cvec.fit_transform(X_train).todense(), columns=cvec.get_feature_names())\n",
    "X_test  = pd.DataFrame(cvec.transform(X_test).todense(),      columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "source": [
    "The baseline accuracy for this model is about 50% because one could simply guess 1 or 0 for all of the rows and get 50% correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Neïve Bayes\n",
      "Train: 0.9263513513513514\n",
      "Test: 0.8663967611336032\n",
      "\n",
      "Extra Trees\n",
      "Train: 0.9898648648648649\n",
      "Test: 0.8016194331983806\n",
      "\n",
      "Logistic Regression\n",
      "Train: 0.977027027027027\n",
      "Test: 0.8421052631578947\n",
      "\n",
      "Gradient Boost\n",
      "Train: 0.8722972972972973\n",
      "Test: 0.7975708502024291\n",
      "\n",
      "K Nearest Neighbors\n",
      "Train: 0.7763513513513514\n",
      "Test: 0.6497975708502024\n",
      "\n",
      "Support Vector Machine\n",
      "Train: 0.6054054054054054\n",
      "Test: 0.6376518218623481\n"
     ]
    }
   ],
   "source": [
    "multi_model = MultinomialNB().fit(X_train,y_train)\n",
    "print(\"Multinomial Neïve Bayes\")\n",
    "print(\"Train:\", multi_model.score(X_train,y_train))\n",
    "print(\"Test:\", multi_model.score(X_test, y_test))\n",
    "print(\"\")\n",
    "extra_trees = ExtraTreesClassifier().fit(X_train, y_train)\n",
    "print(\"Extra Trees\")\n",
    "print(\"Train:\", extra_trees.score(X_train,y_train))\n",
    "print(\"Test:\", extra_trees.score(X_test,y_test))\n",
    "print(\"\")\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Train:\", log_reg.score(X_train,y_train))\n",
    "print(\"Test:\", log_reg.score(X_test,y_test))\n",
    "print(\"\")\n",
    "gradient = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "print(\"Gradient Boost\")\n",
    "print(\"Train:\", gradient.score(X_train,y_train))\n",
    "print(\"Test:\", gradient.score(X_test,y_test))\n",
    "print(\"\")\n",
    "KNN = KNeighborsClassifier().fit(X_train, y_train)\n",
    "print(\"K Nearest Neighbors\")\n",
    "print(\"Train:\", KNN.score(X_train,y_train))\n",
    "print(\"Test:\", KNN.score(X_test,y_test))\n",
    "print(\"\")\n",
    "support = SVC().fit(X_train, y_train)\n",
    "print(\"Support Vector Machine\")\n",
    "print(\"Train:\", support.score(X_train,y_train))\n",
    "print(\"Test:\", support.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
